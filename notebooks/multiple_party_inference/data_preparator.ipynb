{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Image Data preparator\n",
    "### author: Oleksandr Lytvyn\n",
    "#### Tasks\n",
    "1. Upload available images\n",
    "2. Create a dataset\n",
    "3. Perform data augmentation\n",
    "4. Save data in corresponding folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x2dfea176e30>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import os\n",
    "torch.manual_seed(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# define dataset class\n",
    "class BrainMRIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_directory, reshape=True, height=128, width=128, transform=None):\n",
    "        self.data_directory = data_directory\n",
    "        self.no_class = glob(self.data_directory + '/no/*')\n",
    "        self.yes_class = glob(self.data_directory + '/yes/*')\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.reshape=reshape\n",
    "        self.transform = transform\n",
    "\n",
    "        labels = [0 for i in range(len(self.no_class))]\n",
    "        labels += [1 for i in range(len(self.yes_class))]\n",
    "\n",
    "        image_links = self.no_class + self.yes_class\n",
    "        self.dataframe = pd.DataFrame({\n",
    "            'image':image_links,\n",
    "            'labels': labels\n",
    "        })\n",
    "\n",
    "        self.dataframe = shuffle(self.dataframe)\n",
    "        self.dataframe.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.no_class)+len(self.yes_class)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        image = self.dataframe['image'][idx]\n",
    "        label = self.dataframe['labels'][idx]\n",
    "\n",
    "        image = Image.open(image).convert(\"L\")\n",
    "\n",
    "        if self.reshape:\n",
    "            image = image.resize((self.height,self.width))\n",
    "\n",
    "        array = np.asarray(image)\n",
    "        if self.transform:\n",
    "            array = self.transform(array)\n",
    "\n",
    "        array = array.reshape(1, self.height,self.width)\n",
    "        image = torch.tensor(array)\n",
    "        label = torch.tensor(label)\n",
    "\n",
    "        return [image,label]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.dataframe.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# define transformation\n",
    "color_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(brightness=0.4),\n",
    "        transforms.ColorJitter(brightness=0.5),\n",
    "        transforms.ColorJitter(brightness=0.6),\n",
    "        transforms.ColorJitter(brightness=0.7)\n",
    "    ]),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "rotation_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomRotation(degrees=30),\n",
    "        transforms.RandomRotation(degrees=25),\n",
    "        transforms.RandomRotation(degrees=20),\n",
    "        transforms.RandomRotation(degrees=15)\n",
    "    ]),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "flip_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomHorizontalFlip(p=1),\n",
    "        transforms.RandomVerticalFlip(p=1),\n",
    "        transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=1),\n",
    "            transforms.RandomVerticalFlip(p=1)\n",
    "        ])\n",
    "    ]),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "grayscale_transformations = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomChoice([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.RandomPerspective(distortion_scale=.1, p=1)\n",
    "    ]),\n",
    "    transforms.ToTensor()\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#upload data, create dataset and perform transformation\n",
    "path_to_project_root = '../../'\n",
    "data_dir = path_to_project_root + 'data/brain_tumor_imgs'\n",
    "dataset = BrainMRIDataset(data_dir, height=64, width=64, transform=None) + \\\n",
    "          BrainMRIDataset(data_dir, height=64, width=64, transform=rotation_transformations) + \\\n",
    "          BrainMRIDataset(data_dir, height=64, width=64, transform=flip_transformations) + \\\n",
    "          BrainMRIDataset(data_dir, height=64, width=64, transform=grayscale_transformations) + \\\n",
    "          BrainMRIDataset(data_dir, height=64, width=64, transform=color_transformations)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole dataset size: 1265\n",
      "Train dataset size: 1013\n",
      "Test, DO1 and DO2 sizes: 84\n"
     ]
    }
   ],
   "source": [
    "#split create data\n",
    "datasetsize = len(dataset)\n",
    "train_size = int(datasetsize * 0.8)\n",
    "whole_test_size = datasetsize - train_size\n",
    "single_part_size = int(whole_test_size/3) #whole test data is divided into 3 parts - for test, do1 and do2\n",
    "\n",
    "if datasetsize != (train_size + single_part_size*3):\n",
    "    train_size += 1\n",
    "\n",
    "print(f\"Whole dataset size: {datasetsize}\\n\"\n",
    "      f\"Train dataset size: {train_size}\\n\"\n",
    "      f\"Test, DO1 and DO2 sizes: {single_part_size}\")\n",
    "\n",
    "train_data, test_data, do1_data, do2_data = torch.utils.data.random_split(dataset, [train_size, single_part_size, single_part_size, single_part_size])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ds/train/imgs', 'ds/train/labels', 'ds/test/imgs', 'ds/test/labels', 'do1/imgs', 'do1/labels', 'do2/imgs', 'do2/labels']\n"
     ]
    }
   ],
   "source": [
    "#create corresponding dirs for data scientist (ds), data owner 1 (do1) and data owner 2 (do2)\n",
    "path_to_project_root = '../../'\n",
    "main_image_dir = path_to_project_root + f'data/prepared_brain_tumor_imgs/'\n",
    "all_dir_names = []\n",
    "party_names = ['ds', 'do1', 'do2']\n",
    "\n",
    "for name in party_names:\n",
    "    party_dir_names = [f'{name}/imgs', f'{name}/labels']\n",
    "    if name == 'ds':\n",
    "        party_dir_names = [f'{name}/train/imgs',\n",
    "                     f'{name}/train/labels',\n",
    "                     f'{name}/test/imgs',\n",
    "                     f'{name}/test/labels']\n",
    "\n",
    "    for dir_name in party_dir_names:\n",
    "        os.makedirs(main_image_dir + dir_name, exist_ok=True)\n",
    "\n",
    "    all_dir_names += party_dir_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-6af9a82cd560>:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(array)\n"
     ]
    }
   ],
   "source": [
    "# save created data in corresponding libraries\n",
    "for i, data in enumerate(train_data):\n",
    "    torch.save(data[0], main_image_dir + f'ds/train/imgs/img{i}')\n",
    "    torch.save(data[1], main_image_dir + f'ds/train/labels/lbl{i}')\n",
    "\n",
    "for i, data in enumerate(test_data):\n",
    "    torch.save(data[0], main_image_dir + f'ds/test/imgs/img{i}')\n",
    "    torch.save(data[1], main_image_dir + f'ds/test/labels/lbl{i}')\n",
    "\n",
    "for i, data in enumerate(do1_data):\n",
    "    torch.save(data[0], main_image_dir + f'do1/imgs/img{i}')\n",
    "    torch.save(data[1], main_image_dir + f'do1/labels/lbl{i}')\n",
    "\n",
    "for i, data in enumerate(do2_data):\n",
    "    torch.save(data[0], main_image_dir + f'do2/imgs/img{i}')\n",
    "    torch.save(data[1], main_image_dir + f'do2/labels/lbl{i}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}