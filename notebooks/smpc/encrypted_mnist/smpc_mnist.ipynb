{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f81a682d",
   "metadata": {},
   "source": [
    "# Encrypted Computation on MNIST using SyMPC\n",
    "#### author: Oleksandr Lytvyn\n",
    "CONTEXT\n",
    "One party has a CNN model trained on MNIST dataset, other party wants\n",
    "to make predictions on the trained model. First party do not want to share\n",
    "model hyper-parameters, weights, etc.\n",
    "\n",
    "EXPERIMENT SCENARIO\n",
    "1. Prepare Data:\n",
    "    1. download dataset\n",
    "    2. train_data - first party\n",
    "    3. test_data - second party\n",
    "2. Spread data between parties\n",
    "3. CNN definition and Training\n",
    "4. Encrypted Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f1225a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfddceb5",
   "metadata": {},
   "source": [
    "#### Load MNIST dataset and prepare Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4490f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'train': <torch.utils.data.dataloader.DataLoader at 0x18a4b8be940>,\n 'test': <torch.utils.data.dataloader.DataLoader at 0x18a4df6c280>}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True,\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "loaders = {\n",
    "    'train' : torch.utils.data.DataLoader(train_data,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1),\n",
    "\n",
    "    'test'  : torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1),\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "2ab9366c",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "779af31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class CNN(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(CNN, self).__init__(torch_ref=torch_ref)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        self.relu2=nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CNN object at 0x0000018A541A90A0>\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(torch)\n",
    "print(cnn)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "49ce2b8b",
   "metadata": {},
   "source": [
    "#### Define Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ade749ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr = 0.01)\n",
    "\n",
    "def train(num_epochs, cnn, loaders):\n",
    "\n",
    "    cnn.train()\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "\n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)   # batch x\n",
    "            b_y = Variable(labels)   # batch y\n",
    "            output = cnn(b_x)[0]\n",
    "            loss = loss_func(output, b_y)\n",
    "\n",
    "            # clear gradients for this training step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backpropagation, compute gradients\n",
    "            loss.backward()\n",
    "            # apply gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                       .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "                pass\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97352c8",
   "metadata": {},
   "source": [
    "#### Perform Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "639f92ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1256\n",
      "Epoch [1/5], Step [200/600], Loss: 0.0542\n",
      "Epoch [1/5], Step [300/600], Loss: 0.1522\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0289\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1084\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1309\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0120\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0373\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0070\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0669\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1413\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0787\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0250\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0369\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0147\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0085\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0591\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0340\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0086\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0323\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0084\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0142\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0395\n",
      "Epoch [4/5], Step [600/600], Loss: 0.1118\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0144\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0213\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0422\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0873\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0071\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0066\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train(num_epochs, cnn, loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5526a",
   "metadata": {},
   "source": [
    "#### Perform Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dd432d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 1.00\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    # Test the model\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in loaders['test']:\n",
    "            test_output, last_layer = cnn(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "            pass\n",
    "    print('Test Accuracy of the model on the 10000 test images: %.2f' % accuracy)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "10000"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-41-8ca0f82f9824>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtest_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtest_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m1000\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\pysyft\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    103\u001B[0m             \u001B[0mtuple\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m \u001B[0mwhere\u001B[0m \u001B[0mtarget\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0mindex\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mtarget\u001B[0m \u001B[1;32mclass\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    104\u001B[0m         \"\"\"\n\u001B[1;32m--> 105\u001B[1;33m         \u001B[0mimg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtargets\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    106\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m         \u001B[1;31m# doing this so that it is consistent with all other datasets\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "a100cc5d",
   "metadata": {},
   "source": [
    "### Encrypted inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c1a5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import sympc\n",
    "from sympc.session import Session\n",
    "from sympc.session import SessionManager\n",
    "from sympc.tensor import MPCTensor\n",
    "from sympc.protocol import FSS\n",
    "from sympc.protocol import Falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30792e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clients(n_parties):\n",
    "  #Generate required number of syft clients and return them.\n",
    "\n",
    "  parties=[]\n",
    "  for index in range(n_parties): \n",
    "      parties.append(sy.VirtualMachine(name = \"worker\"+str(index)).get_root_client())\n",
    "\n",
    "  return parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67c45b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_send(data,session):\n",
    "    \"\"\"Splits data into number of chunks equal to number of parties and distributes it to respective \n",
    "       parties.\n",
    "    \"\"\"\n",
    "    data_pointers = []\n",
    "    \n",
    "    split_size = int(len(data)/len(session.parties))+1\n",
    "    for index in range(0,len(session.parties)):\n",
    "        ptr=data[index*split_size:index*split_size+split_size].share(session=session)\n",
    "        data_pointers.append(ptr)\n",
    "        \n",
    "    return data_pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fe0d857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def inference(n_clients, model,protocol=None):\n",
    "    \n",
    "  # Get VM clients \n",
    "  parties=get_clients(n_clients)\n",
    "\n",
    "  # Setup the session for the computation\n",
    "  if(protocol):\n",
    "     session = Session(parties = parties,protocol = protocol)\n",
    "  else:\n",
    "     session = Session(parties = parties)\n",
    "        \n",
    "  SessionManager.setup_mpc(session)\n",
    "\n",
    "  #Split data and send data to clients\n",
    "  imgs, lbls = next(iter(loaders['test']))\n",
    "  actual_number = lbls[:10].numpy()\n",
    "  pointers = split_send(imgs,session)\n",
    "\n",
    "  #Encrypt model \n",
    "  mpc_model = model.share(session)\n",
    "\n",
    "  #Encrypt test data\n",
    "  #test_data=MPCTensor(secret=test_x, session = session)\n",
    "\n",
    "  #Perform inference and measure time taken\n",
    "  start_time = time.time()\n",
    "    \n",
    "  results = []\n",
    "    \n",
    "  for ptr in pointers:\n",
    "     encrypted_results = mpc_model(ptr)\n",
    "     plaintext_results = encrypted_results.reconstruct()\n",
    "     results.append(plaintext_results)\n",
    "        \n",
    "  end_time = time.time()\n",
    "\n",
    "  print(f\"Time for inference: {end_time-start_time}s\")\n",
    "    \n",
    "\n",
    "  pred_y = torch.max(results, 1)[1].data.numpy().squeeze()\n",
    "  print(f'Prediction number: {pred_y}')\n",
    "  print(f'Actual number: {actual_number}')\n",
    "    \n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07db1913",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ReLU'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-83-f5effd29e18a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mpredictions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minference\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcnn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mFalcon\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"semi-honest\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-82-12f4a42e4654>\u001B[0m in \u001B[0;36minference\u001B[1;34m(n_clients, model, protocol)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m   \u001B[1;31m#Encrypt model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m   \u001B[0mmpc_model\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshare\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msession\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m   \u001B[1;31m#Encrypt test data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\pysyft\\lib\\site-packages\\sympc\\module\\__init__.py\u001B[0m in \u001B[0;36mshare\u001B[1;34m(_self, session)\u001B[0m\n\u001B[0;32m     75\u001B[0m             \u001B[0mmpc_module\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_modules\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 77\u001B[1;33m             \u001B[0msympc_type_layer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMAP_TORCH_TO_SYMPC\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname_layer\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     78\u001B[0m             \u001B[0msympc_layer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msympc_type_layer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msession\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msession\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     79\u001B[0m             \u001B[0madditional_attributes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcopy_additional_attributes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodule\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname_layer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'ReLU'"
     ]
    }
   ],
   "source": [
    "predictions=inference(3, cnn, Falcon(\"semi-honest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97a4c36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}